{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc490194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#since Y = WX\n",
    "\n",
    "class LinearRegression2:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self , X , y):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack([np.ones((n , 1)) , X])\n",
    "        self.w = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    \n",
    "    def predict(self , X):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack([X , np.ones((n , 1)) ])\n",
    "        y = X @ self.w\n",
    "        return y\n",
    "\n",
    "\n",
    "# Create example input data\n",
    "X = np.array([[2, 2], [4, 5], [7, 8]])\n",
    "y = np.array([9, 17, 26])\n",
    "\n",
    "lr = LinearRegression2()\n",
    "lr.fit(X , y)\n",
    "print(lr.w)\n",
    "\n",
    "y_pred = lr.predict(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03759009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self , lr = 0.001 , n_iters = 10):\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.n_iters = n_iters\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self , X , Y  ):\n",
    "\n",
    "        n_samples , n_features  = X.shape\n",
    "        self.weight = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            z = np.dot(X , self.weight) + self.bias\n",
    "            y_pred = self._sigmoid(z)\n",
    "            cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "\n",
    "            dw = (1/n_samples) * (np.dot(X.T , (y_pred - Y)))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - Y)\n",
    "\n",
    "            self.weight -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weight) + self.bias\n",
    "        y_pred = self._sigmoid(z)\n",
    "        return np.round(y_pred).astype(int)\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X, y)\n",
    "\n",
    "X_new = np.array([[6, 7], [7, 8]])\n",
    "y_pred = lr.predict(X_new)\n",
    "\n",
    "print(y_pred)  # [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833489ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self , epochs = 10 , lr=0.0.1):\n",
    "        self.w1 = None\n",
    "        self.b1 = None\n",
    "        self.w2 = None\n",
    "        self.b2 = None\n",
    "\n",
    "    def forward(self , X , Y):\n",
    "        h1 = self.w1 @ X + self.b1\n",
    "        z1 = np.maximum(0 , h1)\n",
    "        h2 = self.w2 @ z2 + self.b2\n",
    "        z2 = np.exp(z2)\n",
    "        loss = np.exp(z2) / np.sum(np.exp())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    def __init__(self , k = 5):\n",
    "        self.k = k\n",
    "    def fit(self , X , Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def predict(self , X_test):\n",
    "        y_pred = []\n",
    "        n , m = X_test.shape[0] , X_test.shape[1]\n",
    "\n",
    "        for x in X_test:\n",
    "            dist = np.linalg.norm(self.X - x , axis = 1)\n",
    "            sort_dist = np.argsort(dist)[:self.k]\n",
    "            labels = self.Y[sort_dist]\n",
    "            label = Counter(labels).most_common(1)[0][0]\n",
    "            y_pred.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KClustering:\n",
    "    def __init__(self , k = 10 , max_iterations = 100):\n",
    "        self.k = k\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def fit(self , X):\n",
    "        self.centroids = X[np.random.choice(len(X) , self.k)]\n",
    "        for i in range(self.max_iterations):\n",
    "            cluster_assignment = []\n",
    "            for j in range(len(X)):\n",
    "                dist = np.linalg.norm(X[j] - self.centroids , axis = 1)\n",
    "                cluster_assignment.append(np.argmin(dist))\n",
    "\n",
    "            for k in range(self.k):\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
